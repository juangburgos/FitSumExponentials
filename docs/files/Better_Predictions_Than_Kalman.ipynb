{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Better Predictions than the Kalman Filter\n\nIn this notebook, an alternative to the Kalman filter is introduced, that offers the following advantages:\n* It performs better when predicting the measured output.\n* It is simpler and more intuitive to tune.\n\nIt is assumed that the reader has basic knowledge of what the Kalman filter is, and what it is used for, as well as the basics of Transfer Functions and State-Space models.\nThis notebook includes simulations that will increase in complexity as features of the Kalman and alternative filters are introduced.\n\nAll the Python code in this notebook is executed directly in the browser using `pyodide`, no data is sent to any server. If simulations take too much time to execute, try running them in a faster computer (wink).\n\nFirst import all the Python packages that will be required for the simulations:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib_inline\nimport numpy as np\nimport scipy.signal as sig\nfrom scipy.linalg import solve_discrete_are as dare\nimport matplotlib.pyplot as plt\nimport math\nimport pyarma as pa\nfrom ipywidgets import interact\nfrom sympy import symbols, simplify, fraction, Poly",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "A typical second order system is used in the simulations, the continuous time transfer function (in the Laplace domain) is:\n$$\n\\frac{y(s)}{u(s)} = \\frac{k {\\omega}^2}{s^2 + 2 \\xi \\omega s + {\\omega}^2}\n$$\nWhere $s$ is the Laplace variable, $y$ is the system output, $u$ the input $k$ is the gain, $\\xi$ is the [damping factor](https://en.wikipedia.org/wiki/Damping) and $\\omega$ is the natural frequency.\n\nTo be able to simulate it, it is first defined as a continuous time transfer function and then converted to state-space form using `scipy.signal`'s `tf2ss`, which yields the continuous time state-space model:\n\n$$\ny(t) = C_{sys} \\cdot x(t) + D_{sys} \\cdot u(t)\n$$\n$$\n\\dot{x}(t) = A_{sys} \\cdot x(t) + B_{sys} \\cdot u(t)\n$$\nThe transfer function and state-space equations are just different representations of _the same_ system. We use both, because it is easier to design using transfer functions, but it is easier to compute (simulate) using state-space representations (computers like matrices and probably viceversa).\n\nThen the continuous time state-space model is discretized using `scipy.signal`'s `cont2discrete`, and a sampling period `t_step`.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# function that accepts 2rd ord params and returns pyarma discrete state space matrices\ndef second_order_ss_model(k = 41, wn = 0.1, gi = 0.2, t_step = 0.25):\n    # create state space model\n    wn2 = pow(wn, 2)\n    num = [k*wn2]\n    den = [1, 2*gi*wn, wn2]\n    (num, den) = sig.normalize(num, den)\n    sys_ss_c = sig.tf2ss(num, den)\n    \n    # discrete state space model\n    (Ad, Bd, Cd, Dd, _) = sig.cont2discrete(sys_ss_c, t_step)\n    return pa.mat(Ad), pa.mat(Bd), pa.mat(Cd), pa.mat(Dd)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "A couple of functions are defined to create the vectors used to store the simulation results, and the function that is used to display them using `matplotlib`.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def sim_vectors(x_ini, t_step, t_len):\n    # time, input, state, output vectors\n    t = pa.regspace(0, t_step, t_len).t()\n    u = pa.ones(pa.size(t))\n    x = pa.zeros(x_ini.n_rows, t.n_cols)\n    y = pa.zeros(1, t.n_cols)\n    # set initial conditions\n    x[:,0] = x_ini\n    return t, u, x, y\n\ndef sim_plot(t, x_sys, y_sys, x_estk = None, y_estk = None, x_estf = None, y_estf = None, t_line = None):\n    plot_sys = {'color': 'b', 'alpha': 0.75, 'linewidth': 1.2}\n    plot_kal = {'color': 'r', 'alpha': 1, 'linewidth': 1.2}\n    plot_alt = {'color': 'y', 'alpha': 1, 'linewidth': 1.2}\n    legend = {'bbox_to_anchor': (1.001, 0), 'loc': 'lower left', 'fontsize': \"8\", 'shadow': True, 'framealpha': 1}\n    # measurement\n    plt.subplot(3, 1, 1)\n    plt.plot(t.t(), y_sys.t(), **plot_sys, label='y_sys (measurement)')\n    if y_estk is not None:\n        plt.plot(t.t(), y_estk.t(), **plot_kal, label='y_estk (kalman)')\n    if y_estf is not None:\n        plt.plot(t.t(), y_estf.t(), **plot_alt, label='y_estf (alternative)')\n    plt.legend(**legend)\n    plt.ylim((0, 120))\n    plt.grid(alpha=0.3)\n    if t_line is not None:\n        plt.axvline(x=t_line, **plot_sys, linestyle=\"--\")\n    # state 1\n    plt.subplot(3, 1, 2)\n    plt.plot(t.t(), x_sys[0,:].t(), **plot_sys, label='x1_sys (state 1)')\n    if x_estk is not None:\n        plt.plot(t.t(), x_estk[0,:].t(), **plot_kal, label='x1_estk (state 1 kalman)')\n    if x_estf is not None:\n        plt.plot(t.t(), x_estf[0,:].t(), **plot_alt, label='x1_estf (state 1 alternative)')\n    plt.legend(**legend)\n    plt.ylim((-40, 60))\n    plt.grid(alpha=0.3)\n    if t_line is not None:\n        plt.axvline(x=t_line, **plot_sys, linestyle=\"--\")\n    # state 2\n    plt.subplot(3, 1, 3)\n    plt.plot(t.t(), x_sys[1,:].t(), **plot_sys, label='x2_sys (state 2)')\n    if x_estk is not None:\n        plt.plot(t.t(), x_estk[1,:].t(), **plot_kal, label='x2_estk (state 2 kalman)')\n    if x_estf is not None:\n        plt.plot(t.t(), x_estf[1,:].t(), **plot_alt, label='x2_estf (state 2 alternative)')\n    plt.legend(**legend)\n    plt.ylim((0, 200))\n    plt.grid(alpha=0.3)\n    if t_line is not None:\n        plt.axvline(x=t_line, **plot_sys, linestyle=\"--\")\n    plt.xlabel('t')\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The simulation of the second order system response is just a simple `for` loop evaluating the discrete time state-space equations of the model:\n$$\ny[n] = C_{dsys} \\cdot x[n] + D_{dsys} \\cdot u[n]\n$$\n$$\nx[n+1] = A_{dsys} \\cdot x[n] + B_{dsys} \\cdot u[n]\n$$\nWhere $x$ is the state _vector_ of the model and $n$ is just the index in the corresponding array. Since the model is discretized at a sampling period of `t_step`, then $y[n]$ is the system output at time `n*t_step`, $x[n]$ is the system state at time `n*t_step` and $x[n+1]$ is the system state at time `(n+1)*t_step` .\n\nThe $dsys$ subscript in the state-space matrices denotes that they belong to the _discretized system_.\n\nThe system's state values at time zero, $x[0]$, are the system's _initial conditions_.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def sim_second_order(\n    k = 41,           # gain\n    wn = 0.1,         # natural frequency\n    gi = 0.2,         # damping factor\n    x1_sys_ini = 1,   # initial condition for state 1\n    x2_sys_ini = 150, # initial condition for state 2\n):\n    # time step, time length\n    t_step = 0.25; t_len = 200\n    # discrete system state space model\n    (Ad_sys, Bd_sys, Cd_sys, Dd_sys) = second_order_ss_model(k, wn, gi, t_step)    \n    # time, input, system state and output vectors\n    (t, u, x_sys, y_sys) = sim_vectors(pa.mat([x1_sys_ini, x2_sys_ini]).t(), t_step, t_len)\n    \n    # run simulation\n    for n in range(0, u.n_cols):\n        # system output equation\n        y_sys[:,n] = Cd_sys * x_sys[:,n] + Dd_sys * u[:,n]\n        # system state equation\n        if n < u.n_cols - 1:\n            x_sys[:,n+1] = Ad_sys * x_sys[:,n] + Bd_sys * u[:,n]\n\n    # show simulation results\n    sim_plot(t, x_sys, y_sys)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The simulation results are displayed using `ipywidgets` which allows to interactivelly discover how the system output changes with respect to it's parameters `k` ($k$), `wn` ($\\omega$) and `gi` ($\\xi$), _and_ its initial conditions `x1_sys_ini` and `x2_sys_ini` ($x[0]$).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "interact(\n    sim_second_order, \n    k = (1, 100, 10), \n    wn = (0.1, 0.35, 0.05), \n    gi = (0.1, 2, 0.1),\n    x1_sys_ini = (0, 10, 1), \n    x2_sys_ini = (100, 200, 5)\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note that in the previous simulation, the system's input $u$ is zero for all time. It is left to the reader, as an excercise, to add the possibility of defining a non-zero input to the simulation.\n\nFor the purpose of showing the Kalman and alternative filter results, the free response of the system with non-zero initial conditions is enough.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## The Kalman Filter\n\nAssume we have a model of the system of interest. That is, in the second order system example, assume we have _estimates_ of $k$, $\\omega$ and $\\xi$, which we will write as $\\hat{k}$, $\\hat{\\omega}$ and $\\hat{\\xi}$ (using the hat to denote _estimate_). How can we estimate the state vector $x$ from measurements of the output $y$? One idea is to simulate the system model in parallel (at the same time) with the real system, and use some algorithm to _align_ the model with the measurements of the system. An algorithm to achieve the alignment of the model and the measurements is the [Luenberger Observer](https://en.wikipedia.org/wiki/State_observer).\n\nWe can obtain an estimate of the discrete time state-space matrices $\\hat{A}_{dsys}$, $\\hat{B}_{dsys}$, $\\hat{C}_{dsys}$ and $\\hat{D}_{dsys}$ using the same procedure as before (`scipy.signal`'s `tf2ss` and `cont2discrete`). The estimates of $x$, which we write as $\\hat{x}$, are obtained with a discrete-time Luenberger which has the form:\n\n$$\n\\hat{y}[n] = \\hat{C}_{dsys} \\cdot \\hat{x}[n] + \\hat{D}_{dsys} \\cdot u[n]\n$$\n$$\n\\hat{x}[n+1] = \\hat{A}_{dsys} \\cdot \\hat{x}[n] + \\hat{B}_{dsys} \\cdot u[n] + L \\left(  y[n] - \\hat{y}[n] \\right)\n$$\n\nThe Luenberger observer has the same form as the system model itself, but with a correction factor in the state equation $L ( y[n] - \\hat{y}[n] )$, where $L$ is the _observer gain_. The problem is to choose an $L$ that guarantees that our estmated states $\\hat{x}[n]$ converge (align) to the real ones $x[n]$. As a note, the continuous time observer has the exact same structure, but with continuous equations.\n\nIt turns out that, if the system model is exact (i.e. $\\hat{A}_{dsys} = A_{dsys}$, $\\hat{B}_{dsys} = B_{dsys}$ and so on), any choice of $L$ that makes the matrix $\\left( A_{dsys} - L \\cdot C_{dsys} \\right)$ have its [eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) inside the unit circle (absolute eigenvalues less than $1$) guarantees that $\\hat{x}[n]$ will eventually converge (align) to $x[n]$ (in continuous time the eigenvalues should be negative). So there are _infinitely_ many $L$s that could solve our problem, so which one to choose?\n\nThis is where Kalman comes in, [the Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter#Underlying_dynamic_system_model) is actually a special case of the Luenberger observer where $L$ is chosen to be the solution if an specific optimization problem. For the Kalman filter, the observer gain $L$ is called $K$ to denote that it is Kalman's specific choice of $L$.\n\nThe details about Kalman's optimization problem are omitted here, except for what is relevant for actual implementation. The problem is typically described in an stochastic framework, and assumes disturbances and noises with probablities expressed matematically by two covariance matrices; $Q$ and $R$ which have the size of the state vector $x$ (squared) and the size of the output $y$ (squared) respectivelly. The solution to that optimization problem can be obtained by solving a [Ricatti Equation](https://en.wikipedia.org/wiki/Riccati_equation) (RE) for the continuous time case and [an Algebraic Riccati Equation](https://en.wikipedia.org/wiki/Algebraic_Riccati_equation) (ARE) for the discrete time case. The ARE depends on the model's state-space matrices and the covariance matrices $Q$ and $R$, which have to be chosen somehow.\n\nSo we have traded the problem of choosing a matrix $L$ for choosing two matrices $Q$ and $R$, great. And we must also solve the (Algebraic) Riccati Equation. Luckily we do not have to solve that equation ourselves, `scipy.linalg`'s  `solve_discrete_are` (imported as `dare`) can do that for us. We can simplify the choice of $Q$ and $R$ by restricting them to be [diagonal](https://en.wikipedia.org/wiki/Diagonal_matrix). Actually to guarantee that the observer converges, $Q$ and $R$ have to be positive-definite (i.e. each diagonal entry of the matrices must be positive).\n\nHere is a Python function that solves the equation and gives us the Kalman gain for given $Q$ and $R$ diagonal entries. We have `q1` and `q2` because the size of $x$ is $2$ for the second order system, and a scalar `r` because the size of $y$ is $1$.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def kalman_gain(Ad, Cd, q1 = 0.5, q2 = 0.5, r = 0.5):\n    Qd = np.array([[q1, 0], [0, q2]])\n    Rd = np.array([[r]])\n    Pd = pa.mat(dare(Ad.t(), Cd.t(), Qd, Rd))\n    Kd = Pd * Cd.t() * pa.pinv(pa.eye(Cd.n_rows, Cd.n_rows) + Cd * Pd * Cd.t())\n    return Kd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Before simulating the Kalman filter, a note about an implementation detail; in practice there is the so called \"Prediction-Correction\" or \"Prediction-Update\" form of the Kalman filter. That is just a fancy name for splitting the observer equations into two parts; the part that includes only the model's equations and the part that includes the correction factor. So the previous equations are re-written as:\n$$\n\\hat{y}[n] = \\hat{C}_{dsys} \\cdot \\hat{x}[n] + \\hat{D}_{dsys} \\cdot u[n]\n$$\n$$\n\\hat{x}[n+1] = \\hat{A}_{dsys} \\cdot \\hat{x}[n] + \\hat{B}_{dsys} \\cdot u[n]\n$$\n$$\n\\hat{x}[n+1] = \\hat{x}[n+1] + L \\left(  y[n] - \\hat{y}[n] \\right)\n$$\nOne advantage of this from is that it allows to disconnect the simulated model from the measurements at any time by omitting the last equation, efectivelly disabling the observer and relying only on the model output.\n\nWithout further ado, here is the function that simulates the Kalman filter running in parallel to the second order system and aligning the state variable estimates $\\hat{x}$.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# simulate kalman filter with given diagonal values for the weight matrices\ndef sim_kalman(q1 = 0.5, q2 = 0.5, r = 0.5, enable = True):\n    # fixed simulation parameters\n    k = 41; wn = 0.1; gi = 0.2; t_step = 0.25; t_len = 200\n    x1_sys_ini = 1; x2_sys_ini = 150\n    \n    # discrete system state space model (the \"real\" system)\n    (Ad_sys, Bd_sys, Cd_sys, Dd_sys) = second_order_ss_model(k, wn, gi, t_step)    \n    # time, input, system state and output vectors\n    (t, u, x_sys, y_sys) = sim_vectors(pa.mat([x1_sys_ini, x2_sys_ini]).t(), t_step, t_len)\n\n    # discrete estimated state space model (the \"simulated\" system)\n    # NOTE : here we assume we know the system perfectly\n    Ad_est = Ad_sys; Bd_est = Bd_sys; Cd_est = Cd_sys; Dd_est = Dd_sys\n    # observer state and output vectors\n    # NOTE : here we assume we do not know the system's initial conditions\n    # the initial guess is a factor of the real initial conditions\n    xerr_factor = 0.75;\n    x1_est_ini = xerr_factor*x1_sys_ini;\n    x2_est_ini = xerr_factor*x2_sys_ini\n    \n    # compute kalman observer gain    \n    Kd = kalman_gain(Ad_est, Cd_est, q1, q2, r)\n    # observer state and output vectors\n    (_, _, x_estk, y_estk) = sim_vectors(pa.mat([x1_est_ini, x2_est_ini]).t(), t_step, t_len)\n    \n    # run simulation\n    for n in range(0, u.n_cols):\n        # system output equation (measurement)\n        y_sys[:,n] = Cd_sys * x_sys[:,n] + Dd_sys * u[:,n]\n        # system state equation\n        if n < u.n_cols - 1:\n            x_sys[:,n+1] = Ad_sys * x_sys[:,n] + Bd_sys * u[:,n]\n        \n        # observer output equation\n        y_estk[:,n] = Cd_est * x_estk[:,n] + Dd_est * u[:,n]\n        # observer state equation\n        if n < u.n_cols - 1:\n            x_estk[:,n+1] = Ad_est * x_estk[:,n] + Bd_est * u[:,n]\n            # correction factor\n            if enable:\n                y_errk = y_sys[:,n] - y_estk[:,n]\n                x_estk[:,n+1] = x_estk[:,n+1] + Kd * y_errk\n\n    # show simulation results\n    sim_plot(t, x_sys, y_sys, x_estk, y_estk)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Again, using `ipywidgets` we can discover how the Kalman filter responds to different values of the covarance matrices $Q$ and $R$, and what happens if we disable the correction term in the Kalman filter equations.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "interact(\n    sim_kalman, \n    q1 = (0.1, 1.0, 0.1),\n    q2 = (0.1, 1.0, 0.1),\n    r = (0.1, 1.0, 0.1),\n    enable = True\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note that the estimated states `x1_estk` and `x1_estk` ($\\hat{x}[n]$ in red), quickly converge (align) to the \"real\" system states `x1_sys` and `x2_sys` ($x[n]$ in blue) respectivelly. It is again left to the reader as an excercise to add the possibility of defining a non-zero input to the simulation.\n\nIt would appear that, in this simple simulation, the covariance matrices somehow influence the \"aggressiveness\" of the Kalman filter, i.e. how fast $\\hat{x}[n]$ converges to $x[n]$, but it is not straght forward to relate the matrices values `q1`, `q2` and `r` to typical filter specifications, like the [bandwidth](https://en.wikipedia.org/wiki/Bandwidth_(signal_processing)). Since the covariance matrices $Q$ and $R$ are supposed to be related to disturbance and noise respectivelly, let's add some to the simulation.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Disturbance and Noise\n\nWith disturbance, it is typically meant a model mismatch (or modelling error). After all, models are a useful simplifications of the real systems, which might be too complicated or expensive to model exactly.\n\nLet's add to the simulation two types of simple disturbances:\n\n* Parametric disturbance : let's assume $\\hat{k}$ is not $k$ anymore, but a factor of it; $\\hat{k} = k_{factor} \\cdot k$.\n* Additive disturbance : let's assume we failed to model a ramp that is added to the system output, which can be modeled with the line equation $ramp = 0.4 \\cdot t$.\n\nLet's also add some random noise to the system output using `numpy`'s `random`.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# simulate kalman filter under different types of disturbance and noise\ndef sim_kalman_disturbance(q1 = 0.5, q2 = 0.5, r = 0.5, dist_k_factor = 1.0, dist_ramp_output = False, noise_output = True):\n    # fixed simulation parameters\n    k = 41; wn = 0.1; gi = 0.2; t_step = 0.25; t_len = 200\n    x1_sys_ini = 1; x2_sys_ini = 150\n    # discrete system state space model\n    # NOTE : here we add parametric disturbance\n    (Ad_sys, Bd_sys, Cd_sys, Dd_sys) = second_order_ss_model(dist_k_factor * k, wn, gi, t_step)\n    # time, input, system state and output vectors\n    (t, u, x_sys, y_sys) = sim_vectors(pa.mat([x1_sys_ini, x2_sys_ini]).t(), t_step, t_len)\n    # noise vector\n    noise = pa.mat(np.random.default_rng(0).random(u.n_cols))\n\n    # discrete estimated state space model\n    (Ad_est, Bd_est, Cd_est, Dd_est) = second_order_ss_model(k, wn, gi, t_step)\n    # assume we do not know the system's initial conditions, initial guess is a factor\n    xerr_factor = 0.75; x1_est_ini = xerr_factor*x1_sys_ini; x2_est_ini = xerr_factor*x2_sys_ini\n    \n    # compute kalman observer gain    \n    Kd = kalman_gain(Ad_est, Cd_est, q1, q2, r)\n    # observer state and output vectors\n    (_, _, x_estk, y_estk) = sim_vectors(pa.mat([x1_est_ini, x2_est_ini]).t(), t_step, t_len)\n    \n    # run simulation\n    for n in range(0, u.n_cols):\n        # system output equation (measurement)\n        y_sys[:,n] = Cd_sys * x_sys[:,n] + Dd_sys * u[:,n]\n        # additive disturbance at the output\n        if dist_ramp_output:\n            y_sys[:,n] = y_sys[:,n] + 0.4 * t[:,n]\n        if noise_output:\n            y_sys[:,n] = y_sys[:,n] + 8.0 * noise[:,n]\n        # system state equation\n        if n < u.n_cols - 1:\n            x_sys[:,n+1] = Ad_sys * x_sys[:,n] + Bd_sys * u[:,n]\n        \n        # observer output equation\n        y_estk[:,n] = Cd_est * x_estk[:,n] + Dd_est * u[:,n]\n        # observer state equation\n        if n < u.n_cols - 1:\n            x_estk[:,n+1] = Ad_est * x_estk[:,n] + Bd_est * u[:,n]\n            # correction factor\n            y_errk = y_sys[:,n] - y_estk[:,n]\n            x_estk[:,n+1] = x_estk[:,n+1] + Kd * y_errk\n\n    # show simulation results\n    sim_plot(t, x_sys, y_sys, x_estk, y_estk)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "With `ipywidgets` we can discover how the Kalman filter responds to different values of $k_{factor}$ and how it responds with and without the additive ramp disturbance and noise.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "interact(\n    sim_kalman_disturbance, \n    q1 = (0.1,1.0, 0.1),\n    q2 = (0.1, 1.0, 0.1),\n    r = (0.1, 1.0, 0.1),\n    dist_k_factor = (0.1, 2, 0.1),\n    dist_ramp_output = False,\n    noise_output = True\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note that this time the estimated states do not converge to the system states, due to the fact that the model and system do not match anymore, as a consequence of adding disturbances and noise. Still, the Kalman filter is able to give reasonable estimates based on the available measurements. This property of the filter performing correctly under disturbances is called _robustness_, so the Kalman filter is robust. And it is also now clear that cafeful tuning of the covariance matrices parameters `q1`, `q2` and `r`, is necessary to achieve a good balance between speed of convergance and how noisy the estimates are, although it is still unclear how to tune them.\n\nThere is another usage of the Kalman filter that is very popular, which is making _predictions_. The idea is that, once the estimated states $\\hat{x}[n]$ have converged to the system states $x[n]$ (this can be infered by looking at the output error $\\left( y[n] - \\hat{y}[n] \\right)$), then we can use the estimated states to iterate the model equations, to predict where the system is going to land at some point in the future. Let see how we can implement that in our simulation. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Prediction\n\nTo simulate making predictions with the Kalman filter, the `predict_from` variable is introduced, to allow defining a moment in time from which we wish to disconnect the model from the measurements and see how good the model-based predictons are.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# simulate kalman filter under different types of disturbance\ndef sim_kalman_prediction(q1 = 0.5, q2 = 0.5, r = 0.5, dist_k_factor = 1.0, dist_ramp_output = False, noise_output = True, predict_from = 100):\n    # fixed simulation parameters\n    k = 41; wn = 0.1; gi = 0.2; t_step = 0.25; t_len = 200\n    x1_sys_ini = 1; x2_sys_ini = 150\n    # discrete system state space model\n    # NOTE : here we add parametric disturbance\n    (Ad_sys, Bd_sys, Cd_sys, Dd_sys) = second_order_ss_model(dist_k_factor * k, wn, gi, t_step)\n    # time, input, system state and output vectors\n    (t, u, x_sys, y_sys) = sim_vectors(pa.mat([x1_sys_ini, x2_sys_ini]).t(), t_step, t_len)\n    # noise vector\n    noise = pa.mat(np.random.default_rng(0).random(u.n_cols))\n\n    # discrete estimated state space model\n    (Ad_est, Bd_est, Cd_est, Dd_est) = second_order_ss_model(k, wn, gi, t_step)\n    # assume we do not know the system's initial conditions, initial guess is a factor\n    xerr_factor = 0.75; x1_est_ini = xerr_factor*x1_sys_ini; x2_est_ini = xerr_factor*x2_sys_ini\n    \n    # compute kalman observer gain    \n    Kd = kalman_gain(Ad_est, Cd_est, q1, q2, r)\n    # observer state and output vectors\n    (_, _, x_estk, y_estk) = sim_vectors(pa.mat([x1_est_ini, x2_est_ini]).t(), t_step, t_len)\n    \n    # run simulation\n    for n in range(0, u.n_cols):\n        # system output equation (measurement)\n        y_sys[:,n] = Cd_sys * x_sys[:,n] + Dd_sys * u[:,n]\n        # additive disturbance at the output\n        if dist_ramp_output:\n            y_sys[:,n] = y_sys[:,n] + 0.4 * t[:,n]\n        if noise_output:\n            y_sys[:,n] = y_sys[:,n] + 8.0 * noise[:,n]\n        # system state equation\n        if n < u.n_cols - 1:\n            x_sys[:,n+1] = Ad_sys * x_sys[:,n] + Bd_sys * u[:,n]\n        \n        # observer output equation\n        y_estk[:,n] = Cd_est * x_estk[:,n] + Dd_est * u[:,n]\n        # observer state equation\n        if n < u.n_cols - 1:\n            x_estk[:,n+1] = Ad_est * x_estk[:,n] + Bd_est * u[:,n]\n            # correction factor\n            # NOTE : at time equals t >= predict_from we stop taking measurements into consideration\n            if t[0,n] < predict_from:\n                y_errk = y_sys[:,n] - y_estk[:,n]\n                x_estk[:,n+1] = x_estk[:,n+1] + Kd * y_errk\n\n    # show simulation results\n    sim_plot(t, x_sys, y_sys, x_estk, y_estk, None, None, predict_from)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Using the `ipywidgets` slider for the `predict_from` variable, we can discover how the predictions perform depending on the point in time that we wish to start the simulations and what disturbances are present. Try disabling all disturbances and noise and compare the results when they are enabled.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "interact(\n    sim_kalman_prediction, \n    q1 = (0.1, 1.0, 0.1),\n    q2 = (0.1, 1.0, 0.1),\n    r = (0.1, 1.0, 0.1),\n    dist_k_factor = (0.1, 2, 0.1),\n    dist_ramp_output = False,\n    noise_output = True,\n    predict_from = (10, 180, 10)\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note that when the disturbances and noise are disabled, predictions are perfect. This is because:\n1. In the absence of disturbances and noise, the model and system match perfectly.\n2. At the point where we start predicting (`predict_from`), the estimated states have already converged to the system states.\n\nIf the disturbances and noise are enabled, the Kalman filter tracks the measured output just until the defined `predict_from`, but afterwards the output predictions become useless. This means that the Kalman filter is robust while the correction factor is taken into account, but the predictions of the Kalman filter are not.\n\nLet's see if there is an alternative that can help us solve that problem.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Better Alternative\n\nLet's recall the Luenberger observer structure, of which the Kalman filter is a special case:\n\n$$\n\\hat{y}[n] = \\hat{C}_{dsys} \\cdot \\hat{x}[n] + \\hat{D}_{dsys} \\cdot u[n]\n$$\n$$\n\\hat{x}[n+1] = \\hat{A}_{dsys} \\cdot \\hat{x}[n] + \\hat{B}_{dsys} \\cdot u[n] + L \\left(  y[n] - \\hat{y}[n] \\right)\n$$\n\nWe have seen so far that the main job of the an observer is keeping the output error $\\left(  y[n] - \\hat{y}[n] \\right)$ small, and we would like to do that under the presence of disturbances and noise. This is starting to sound like a typical _tracking_ [control](https://en.wikipedia.org/wiki/Control_theory) problem. We wish $\\hat{y}[n]$ to _track_ $y[n]$ with certain robustness and performance requirements.\n\nSo why not change the static gain $L$ of the Luenberger observer for a filter $F(s)$? _After all, why not? Why shouldn't we do it?_ [**It can be shown**](https://folk.ntnu.no/skoge/prost/proceedings/ifac2014/media/files/2032.pdf) that the resulting observer structure is, unsurprisingly, another special Luenberger observer. One that includes the filter states $x_{flt}$ and the filter's state-space matrices  $A_{dflt}$, $B_{dflt}$, $C_{dflt}$ and $D_{dflt}$. \n\nIf we include $\\hat{B}_{dsys}$ in the feedback loop of the observer (to avoid having to deal with a multiple inputs in the tracking problem), the observer structure in the prediction-correction form, would be the following:\n\n$$\n\\hat{y}[n] = \\hat{C}_{dsys} \\cdot \\hat{x}[n] + \\hat{D}_{dsys} \\cdot u[n]\n$$\nThere are two state equations for the predition step:\n$$\nx_{flt}[n+1] = A_{dflt} \\cdot x_{flt}[n]\n$$\n$$\n\\hat{x}[n+1] = \\hat{A}_{dsys} \\cdot \\hat{x}[n] + \\hat{B}_{dsys} \\cdot u[n] + \\left( \\hat{B}_{dsys} \\cdot C_{dflt} \\right) \\cdot x_{flt}[n]\n$$\nAnd two equations for the correction step:\n$$\nx_{flt}[n+1] = x_{flt}[n+1] + B_{dflt} \\cdot \\left( y[n] - \\hat{y}[n] \\right)\n$$\n$$\n\\hat{x}[n+1] = \\hat{x}[n+1] + \\left( \\hat{B}_{dsys} \\cdot D_{dflt} \\right) \\cdot \\left( y[n] - \\hat{y}[n] \\right)\n$$\n\nNow there is the problem of selecting the filter $F(s)$, luckily, control theory has left us quite [a few options](https://en.wikipedia.org/wiki/Control_theory#See_also) over the last decades. Some examples:\n* [Pole Placement](https://en.wikipedia.org/wiki/Full_state_feedback).\n* [Loop Shaping](https://en.wikipedia.org/wiki/H-infinity_loop-shaping).\n* [Internal Model Control](https://en.wikipedia.org/wiki/Internal_model_(motor_control)) (IMC).\n\nWe could even use a [PID controller](https://pidtuner.com), but for didactic puposes and simplicity we will _cheat_ and design an IMC filter that will use the inverted model dynamics. With \"_cheat_\" it means that, if the model would be unstable or [non-minimum phase](https://en.wikipedia.org/wiki/Minimum_phase), then inverting the dynamics would [not be a good idea](https://www.youtube.com/watch?v=G9apWx4iaks). We know we can get away with this, because our second order system example is both stable and minimum phase. Otherwise, it would be better to design $F(s)$ using loop shaping techniques.\n\nThe working principles of the IMC method are very simple (for example to [tune a PID controller](https://folk.ntnu.no/skoge/publications/2001/tuningpaper_reno/tuningpaper_06nov01.pdf)):\n* Obtain a model of the system $G(s)$ (we already have that).\n* Specify the desired closed loop dynamics $D(s)$.\n* Use [the formula](https://en.wikipedia.org/wiki/Closed-loop_transfer_function) for the closed loop transfer function to calculate $F(s) = \\frac{1}{G(s)} \\cdot \\frac{1}{D^{-1}(s) - 1}$.\n\nSo now we must simply define how we want the closed loop observer to behave ($D(s)$), typically as a [low pass filter](https://en.wikipedia.org/wiki/Low-pass_filter) where we can specify a pole $\\tau$ that is directly related to the filter's bandwith:\n\n$$\nD(s) = \\frac{\\tau}{s + \\tau}\n$$\n\nIt is left as an exercise to show that this choice of $D(s)$ will lead to a [non-proper](https://en.wikipedia.org/wiki/Proper_transfer_function) transfer function for $F(s)$. Therefore we must add another pole to $D(s)$, at frequencies higher than $\\tau$, for example:\n\n$$\nD(s) = \\frac{\\tau}{s + \\tau} \\cdot \\frac{10 \\cdot \\tau}{s + 10 \\cdot \\tau}\n$$\n\nThis choice of $D(s)$ should yield a proper $F(s)$. We could do the algebra, but instead let's `sympy` do it for us:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# accepts 2rd ord params and desired closed loop pole (tau)\n# and returns the filter's pyarma discrete state space matrices\ndef filter_ss_model(k_val = 41, wn_val = 0.1, gi_val = 0.2, tau_val = 0.2, t_step = 0.25):\n    tk_val = 10\n    # design filter F based on plant model G and desired dynamics D\n    s, k, wn, gi, tau, tk = symbols(\"s k wn gi tau tk\")\n    G = (k * wn**2) / (s**2 + 2*gi*wn*s + wn**2)\n    D = (tau * (tk*tau)) / ((s+tau)*(s+tk*tau))\n    F = (1/G) * (1 / (D**-1 - 1))\n    num, den = fraction(simplify(F))\n    num = num.subs({k: k_val, wn: wn_val, gi: gi_val, tau: tau_val, tk: tk_val}).as_poly(s).all_coeffs()\n    den = den.subs({k: k_val, wn: wn_val, gi: gi_val, tau: tau_val, tk: tk_val}).as_poly(s).all_coeffs()\n    num = [ float(x) for x in num ]\n    den = [ float(x) for x in den ]\n    # create state space model\n    (num, den) = sig.normalize(num, den)\n    filt_ss_c = sig.tf2ss(num, den)\n    # discrete state space model\n    (Ad, Bd, Cd, Dd, _) = sig.cont2discrete(filt_ss_c, t_step)\n    return pa.mat(Ad), pa.mat(Bd), pa.mat(Cd), pa.mat(Dd)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "As an exercise for the reader, verify that the $F(s)$ above contains an [integrator](https://en.wikipedia.org/wiki/Integrator).\n\nBelow is the function implementing the simulation that includes the comparison of both Kalman filter and alternative filter.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# simulate alternative filter under different types of disturbance\ndef sim_filter_prediction(q1 = 0.5, q2 = 0.5, r = 0.5, dist_k_factor = 1.0, dist_ramp_output = False, noise_output = True, predict_from = 100, tau = 0.2):\n    # fixed simulation parameters\n    t_step = 0.25; t_len = 200\n    k = 41; wn = 0.1; gi = 0.2;\n    x1_sys_ini = 1; x2_sys_ini = 150\n    # discrete system state space model\n    # NOTE : here we add parametric disturbance\n    (Ad_sys, Bd_sys, Cd_sys, Dd_sys) = second_order_ss_model(dist_k_factor * k, wn, gi, t_step)\n    # time, input, system state and output vectors\n    (t, u, x_sys, y_sys) = sim_vectors(pa.mat([x1_sys_ini, x2_sys_ini]).t(), t_step, t_len)\n    # noise vector\n    noise = pa.mat(np.random.default_rng(0).random(u.n_cols))\n\n    # discrete estimated state space model\n    (Ad_est, Bd_est, Cd_est, Dd_est) = second_order_ss_model(k, wn, gi, t_step)\n    # assume we do not know the system's initial conditions, initial guess is a factor\n    xerr_factor = 0.75; x1_est_ini = xerr_factor*x1_sys_ini; x2_est_ini = xerr_factor*x2_sys_ini\n\n    # compute kalman observer gain    \n    Kd = kalman_gain(Ad_est, Cd_est, q1, q2, r)\n    # observer state and output vectors (kalman)\n    (_, _, x_estk, y_estk) = sim_vectors(pa.mat([x1_est_ini, x2_est_ini]).t(), t_step, t_len)\n    \n    # compute alternative filter\n    (Ad_flt, Bd_flt, Cd_flt, Dd_flt) = filter_ss_model(k, wn, gi, tau, t_step)\n    # observer state and output vectors (alternative)\n    (_, _, x_estf, y_estf) = sim_vectors(pa.mat([x1_est_ini, x2_est_ini]).t(), t_step, t_len)\n    # filter state vector\n    (_, _, x_flt, _) = sim_vectors(pa.zeros(Ad_flt.n_rows,1), t_step, t_len)\n    \n    # run simulation\n    for n in range(0, u.n_cols):\n        # system output equation (measurement)\n        y_sys[:,n] = Cd_sys * x_sys[:,n] + Dd_sys * u[:,n]\n        # additive disturbance at the output\n        if dist_ramp_output:\n            y_sys[:,n] = y_sys[:,n] + 0.4 * t[:,n]\n        if noise_output:\n            y_sys[:,n] = y_sys[:,n] + 8.0 * noise[:,n]\n        # system state equation\n        if n < u.n_cols - 1:\n            x_sys[:,n+1] = Ad_sys * x_sys[:,n] + Bd_sys * u[:,n]\n\n        # observer output equation (kalman)\n        y_estk[:,n] = Cd_est * x_estk[:,n] + Dd_est * u[:,n]\n        # observer state equation (kalman)\n        if n < u.n_cols - 1:\n            x_estk[:,n+1] = Ad_est * x_estk[:,n] + Bd_est * u[:,n]\n            # correction factor (stop correcting at time equals t >= predict_from)\n            if t[0,n] < predict_from:\n                y_errk = y_sys[:,n] - y_estk[:,n]\n                x_estk[:,n+1] = x_estk[:,n+1] + Kd * y_errk\n        \n        # filter output equation (alternative)\n        y_estf[:,n] = Cd_est * x_estf[:,n] + Dd_est * u[:,n]\n        # filter state equation (alternative)\n        if n < u.n_cols - 1:\n            x_flt[:,n+1] = Ad_flt * x_flt[:,n]\n            x_estf[:,n+1] = Ad_est * x_estf[:,n] + Bd_est * u[:,n] + (Bd_est*Cd_flt) * x_flt[:,n]\n            # correction factor (stop correcting at time equals t >= predict_from)\n            if t[0,n] < predict_from:\n                y_errf = y_sys[:,n] - y_estf[:,n]\n                x_flt[:,n+1] = x_flt[:,n+1] + Bd_flt * y_errf\n                x_estf[:,n+1] = x_estf[:,n+1] + (Bd_est*Dd_flt) * y_errf\n\n    # show simulation results\n    sim_plot(t, x_sys, y_sys, x_estk, y_estk, x_estf, y_estf, predict_from)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "In the `ipywidget` below, the only variable to tune for the alternative filter is `tau` ($\\tau$), the lower the value, the lower the bandwidth (smoother but more phase delay), and the higher the value the higher the bandwidth (better tracking but less noise attenuation).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "interact(\n    sim_filter_prediction, \n    q1 = (0.1, 1.0, 0.1),\n    q2 = (0.1, 1.0, 0.1),\n    r = (0.1, 1.0, 0.1),\n    dist_k_factor = (0.1, 2, 0.1),\n    dist_ramp_output = False,\n    noise_output = True,\n    predict_from = (10, 180, 10),\n    tau = (0.05, 1, 0.05)\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note that with the alternative filter we have managed to reduce the number of tuning parameters to just one; $\\tau$. And that it has a straightforward relation to a common filter specification; the bandwidth. Also note that for the parametric disturbance ($\\hat{k} = k_{factor} \\cdot k$), the alternative filter is able to produce much better output predictions. And for the case of the ramp disturbance, the alternative's filter predictions improve as the bandwidth ($\\tau$) is increased, which agrees with what we would expect from control theory.\n\n## Q & A\n\n* This example is simple because it is a SISO system (Single Input Single Output), what if I have a MIMO system (Multiple Input/Output)?\n\nThe loopshaping problem has been solved for MIMO systems, actually it is quite auotmated now a days with tools like [Matlab's `loopsyn`](https://www.mathworks.com/help/robust/ref/dynamicsystem.loopsyn.html) (sadly not in Python at the time of writing). You just specify the open-loop desired dynamics and the rountine does the rest.\n\n* This looks cool but does it work for real life applications? Given the improved predictions, could I use it for control (e.g. with [Model Predictive Control](https://en.wikipedia.org/wiki/Model_predictive_control), aka MPC)?\n\nYes and yes, in [this paper](https://folk.ntnu.no/skoge/prost/proceedings/ifac2014/media/files/2032.pdf), this very technique was used in a real drone to estimate and predict the states of an MPC controller.\n\n* Does this mean that if my measurements contain some undesired frquency, I could just throw in a notch filter to $F(s)$ and that frequency would dissapear from the state estimates?\n\nYes, this is shown in [this thesis](https://research.tue.nl/nl/studentTheses/disturbance-model-and-observer-tuning-method-for-model-predictive) where this filter was first introduced.",
      "metadata": {}
    }
  ]
}